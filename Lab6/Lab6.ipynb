{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244470eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART I - Ridge Regression\n",
      "\n",
      "Value of lambda: 400 and average RMSE: 0.14951277619075795\n",
      "Value of lambda: 200.0 and average RMSE: 0.14069401845317783\n",
      "Value of lambda: 100.0 and average RMSE: 0.13727677223186574\n",
      "Value of lambda: 50.0 and average RMSE: 0.1361559439275552\n",
      "Value of lambda: 25.0 and average RMSE: 0.13591585919851984\n",
      "Value of lambda: 12.5 and average RMSE: 0.13602237446230686\n",
      "Value of lambda: 6.25 and average RMSE: 0.13626786494076748\n",
      "Value of lambda: 3.125 and average RMSE: 0.13657095693642846\n",
      "Value of lambda: 1.5625 and average RMSE: 0.1368876407319742\n",
      "Value of lambda: 0.78125 and average RMSE: 0.13717626806866692\n",
      "\n",
      "Choose 25.0 for lambda since it provides the lowest avg error.\n",
      "\n",
      "PART II - Linear Regression with Gradient Descent\n",
      "\n",
      "RMSE of Linear Regression Model on Training Data: 0.1285257257429593\n",
      "\n",
      "RMSE of Linear Regression Model on Testing Data: 0.14864482169178425\n",
      "\n",
      "RMSEs of Lab 5: \n",
      "\n",
      "RMSE of Linear Regression Model on Training Data: 0.12768967421762187\n",
      "\n",
      "RMSE of Linear Regression Model on Testing Data: 0.14583464490949136\n",
      "\n",
      "Part III - Ridge regression with 5-fold cross valuation using the gradient descent algorithm\n",
      "\n",
      "Value of lambda: 400 and average RMSE: 0.1495158062061527\n",
      "Value of lambda: 200.0 and average RMSE: 0.14061647291593943\n",
      "Value of lambda: 100.0 and average RMSE: 0.13740362349210433\n",
      "Value of lambda: 50.0 and average RMSE: 0.13605511838669382\n",
      "Value of lambda: 25.0 and average RMSE: 0.13595834814299296\n",
      "Value of lambda: 12.5 and average RMSE: 0.136119302662349\n",
      "Value of lambda: 6.25 and average RMSE: 0.13669035672884403\n",
      "Value of lambda: 3.125 and average RMSE: 0.1362447757829411\n",
      "Value of lambda: 1.5625 and average RMSE: 0.13695627030860255\n",
      "Value of lambda: 0.78125 and average RMSE: 0.13752313478024444\n",
      "\n",
      "Choose 25.0 for lambda since it has the lowest average error for all error rates\n",
      "\n",
      "RMSE for test data using ridge regression with gradient descent is  0.14588707616099433\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lab 6 - COEN 140\n",
    "Marianne Fuyu Yamazaki Dorr\n",
    "10/25/2022\n",
    "\n",
    "Implement Cross Validation and Gradient Descent in Python.  \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def get_data(file):\n",
    "    return pd.read_csv(file ,delimiter='\\t')\n",
    "\n",
    "def RMSE(actual, predicted):\n",
    "    m = len(predicted)\n",
    "    dif = (actual - predicted)\n",
    "    sumsquared = 0\n",
    "    \n",
    "    for x in dif:\n",
    "        sumsquared += x**2\n",
    "    \n",
    "    rmse = math.sqrt(sumsquared/m)\n",
    "    return rmse\n",
    "\n",
    "train = get_data(\"crime-train.txt\")\n",
    "test = get_data (\"crime-test.txt\")\n",
    "\n",
    "#Training Data\n",
    "#add training data except for output to x\n",
    "x_train = train.drop('ViolentCrimesPerPop', axis = 1)\n",
    "#add dummy column of 1s to solve for intercept b\n",
    "dummy = np.ones(len(x_train))\n",
    "x_train = np.column_stack((x_train, dummy))\n",
    "#ensure that data is of correct type float64\n",
    "x_train = np.float64(x_train)\n",
    "\n",
    "#add output data to y, first column of train\n",
    "y_train = train['ViolentCrimesPerPop']\n",
    "#ensure that data is of correct type float64\n",
    "y_train = np.float64(y_train)\n",
    "y_train = np.reshape(y_train, (-1,1))\n",
    "#Testing Data\n",
    "#add testing data except for output to x\n",
    "x_test = test.drop('ViolentCrimesPerPop', axis = 1)\n",
    "dummy2 = np.ones(len(x_test))\n",
    "x_test = np.column_stack((x_test, dummy2))\n",
    "x_test = np.float64(x_test)\n",
    "\n",
    "y_test = test['ViolentCrimesPerPop']\n",
    "y_test = np.float64(y_test)\n",
    "y_test = np.reshape(y_test, (-1,1))\n",
    "\n",
    "\"\"\"\n",
    "PART I - Ridge Regression \n",
    "with k-folding = 5\n",
    "\"\"\"\n",
    "print(\"PART I - Ridge Regression\")\n",
    "print()\n",
    "def ridge_reg(x, y, val, l):\n",
    "    #identity matrix must be of size (p+1) by (p+1)\n",
    "    #(xTx + lambdaI)-1xTy\n",
    "    #use np.identity(size) to create identity matrix\n",
    "    lambda_matrix = l * np.identity(len(x_train.T))\n",
    "    first = np.linalg.inv(np.dot(x.T, x) + lambda_matrix)\n",
    "    second = np.dot(x.T, y)\n",
    "    w = np.dot(first, second)\n",
    "    \n",
    "    pred = []\n",
    "    \n",
    "    for v in val:\n",
    "        pred.append(np.dot(v.T, w))\n",
    "    \n",
    "    return pred\n",
    "\n",
    "l = 400\n",
    "lval = [0]*10\n",
    "errors = np.zeros((10, 5))\n",
    "\n",
    "for x in range(10):\n",
    "    for k in range(5):\n",
    "        x_traink = np.concatenate((x_train[0:int(k*(len(x_train)/5))], x_train[int((k+1)*(len(x_train)/5)):int(len(x_train))]))\n",
    "        y_traink = np.concatenate((y_train[0:int(k*(len(y_train)/5))], y_train[int((k+1)*(len(y_train)/5)):int(len(y_train))]))\n",
    "        \n",
    "        x_valk = x_train[int(k*(len(x_train)/5)):int((k+1)*(len(x_train)/5))]\n",
    "        y_valk = y_train[int(k*(len(y_train)/5)):int((k+1)*(len(y_train)/5))]\n",
    "        \n",
    "        pred = ridge_reg(x_traink, y_traink, x_valk, l)\n",
    "\n",
    "        errors[x, k] = RMSE(pred, y_valk)\n",
    "    lval[x] = l   \n",
    "    l = l/2\n",
    "\n",
    "avg = [0]*10\n",
    "for x in range(10):\n",
    "    sum = 0\n",
    "    for y in range(5):\n",
    "        sum += errors[x][y]\n",
    "    avg[x] = (sum/5)\n",
    "    print(\"Value of lambda: {} and average RMSE: {}\".format(lval[x], avg[x]))\n",
    "\n",
    "minval = min(avg)\n",
    "minposition = avg.index(minval)\n",
    "best_l =  lval[minposition]\n",
    "print()\n",
    "print(\"Choose {} for lambda since it provides the lowest avg error.\".format(lval[minposition]))\n",
    "print()\n",
    "\n",
    "\"\"\"\n",
    "PART II - Linear Regression with Gradient Descent\n",
    "\n",
    "\"\"\"\n",
    "print(\"PART II - Linear Regression with Gradient Descent\")\n",
    "\n",
    "#random w as the initial condition\n",
    "def lin_reg_gd(x, y, w):\n",
    "    w_next = w + (alpha*np.dot(x.T, (y-np.dot(x, w))))\n",
    "    return w_next\n",
    "\n",
    "def lin_reg_loss(x, y, w):\n",
    "    loss = np.dot((y-np.dot(x, w)).T, (y-np.dot(x,w)))\n",
    "    return loss\n",
    "\n",
    "alpha = 0.00001\n",
    "L1 = 1\n",
    "L0 = 0\n",
    "\n",
    "w0 = np.random.normal(0, 1, (x_train.shape[1], 1))\n",
    "\n",
    "while (abs(L1-L0) > (0.00001)):\n",
    "    w1 = lin_reg_gd(x_train, y_train, w0)\n",
    "    L0 = lin_reg_loss(x_train, y_train, w0)\n",
    "    L1 = lin_reg_loss(x_train, y_train, w1)\n",
    "    w0 = w1\n",
    "\n",
    "def problem2(w, m):\n",
    "    #y = xTw\n",
    "    pred = []\n",
    "    for x in m:\n",
    "        pred.append(np.dot(w.T, x))\n",
    "    return pred\n",
    "\n",
    "#find predicted y of training data\n",
    "train_pred = []\n",
    "test_pred = []\n",
    "train_pred = problem2(w0, x_train)\n",
    "print()\n",
    "train_error = RMSE(train_pred, y_train)\n",
    "print(\"RMSE of Linear Regression Model on Training Data: {}\".format(train_error))\n",
    "print()\n",
    "\n",
    "#find predicted y of testing data\n",
    "\n",
    "test_pred = problem2(w0, x_test)\n",
    "test_error = RMSE(test_pred, y_test)\n",
    "print(\"RMSE of Linear Regression Model on Testing Data: {}\".format(test_error))\n",
    "print()\n",
    "\n",
    "print(\"RMSEs of Lab 5: \")\n",
    "print()\n",
    "print(\"RMSE of Linear Regression Model on Training Data: 0.12768967421762187\")\n",
    "print()\n",
    "print(\"RMSE of Linear Regression Model on Testing Data: 0.14583464490949136\")\n",
    "print()\n",
    "\n",
    "\"\"\"\n",
    "PART III - Ridge regression with 5-fold cross valuation using the gradient descent algorithm\n",
    "\"\"\"\n",
    "print(\"Part III - Ridge regression with 5-fold cross valuation using the gradient descent algorithm\")\n",
    "print()\n",
    "def ridge_reg_gd(x, y, w, l):\n",
    "    first = (y - np.dot(x, w)).T\n",
    "    second = (y-np.dot(x, w))\n",
    "    new_w = np.dot(first, second) + (best_l * np.dot(w0.T, w0))\n",
    "    \n",
    "    return new_w\n",
    "\n",
    "alpha = 0.00001\n",
    "\n",
    "def problem3(w, val):\n",
    "    pred = []\n",
    "    for x in val:\n",
    "        pred.append(np.dot(w.T, x))\n",
    "    return pred\n",
    "\n",
    "l = 400\n",
    "lval = [0]*10\n",
    "errors = np.zeros((10, 5))\n",
    "\n",
    "for x in range(10):\n",
    "    for k in range(5):\n",
    "        x_traink = np.concatenate((x_train[0:int(k*(len(x_train)/5))], x_train[int((k+1)*(len(x_train)/5)):int(len(x_train))]))\n",
    "        y_traink = np.concatenate((y_train[0:int(k*(len(y_train)/5))], y_train[int((k+1)*(len(y_train)/5)):int(len(y_train))]))\n",
    "        \n",
    "        x_valk = x_train[int(k*(len(x_train)/5)):int((k+1)*(len(x_train)/5))]\n",
    "        y_valk = y_train[int(k*(len(y_train)/5)):int((k+1)*(len(y_train)/5))]\n",
    "        \n",
    "        L1 = 1\n",
    "        L0 = 0\n",
    "        w0 = np.random.normal(0, 1, (x_train.shape[1],1))\n",
    "        \n",
    "        while (abs(L1-L0) > (0.00001)):\n",
    "            w1 = w0 - (alpha*(np.dot(x_traink.T, (np.dot(x_traink, w0) - y_traink)) + (l * w0)))\n",
    "    \n",
    "            L0 = ridge_reg_gd(x_traink, y_traink, w0, l)\n",
    "            L1 = ridge_reg_gd(x_traink, y_traink, w1, l)\n",
    "            w0 = w1\n",
    "        \n",
    "        val_results = []\n",
    "        val_results = problem3(w0, x_valk)\n",
    "        \n",
    "        errors[x][k] = RMSE(val_results, y_valk)\n",
    "    lval[x] = l   \n",
    "    l = l/2\n",
    "\n",
    "new_avg = [0]*10\n",
    "for x in range(10):\n",
    "    sum = 0\n",
    "    for y in range(5):\n",
    "        sum += errors[x][y]\n",
    "    new_avg[x] = (sum/5)\n",
    "    print(\"Value of lambda: {} and average RMSE: {}\".format(lval[x], new_avg[x]))\n",
    "\n",
    "minval = min(new_avg)\n",
    "minposition = new_avg.index(minval)\n",
    "best_l =  lval[minposition]\n",
    "print()\n",
    "print(\"Choose {} for lambda since it has the lowest average error for all error rates\".format(best_l))\n",
    "print()\n",
    "#for testing data\n",
    "L1 = 1\n",
    "L0 = 0\n",
    "# generate a Gaussian (normal) distribution for our initial value, based off how many features we have\n",
    "w0 = np.random.normal(0, 1, (x_train.shape[1],1))\n",
    "while (abs(L1 - L0) > 0.00001):\n",
    "    w1 = w0 - (alpha * (np.dot(x_train.T, (np.dot(x_train, w0) - y_train)) + (best_l * w0)))\n",
    "    L0 = ridge_reg_gd(x_train, y_train, w0, best_l)\n",
    "    L1 = ridge_reg_gd(x_train, y_train, w1, best_l)\n",
    "    w0 = w1\n",
    "\n",
    "test_pred = []\n",
    "test_pred = problem3(w0, x_test)\n",
    "\n",
    "test_ridge_error = RMSE(test_pred, y_test)\n",
    "print(\"RMSE for test data using ridge regression with gradient descent is \",test_ridge_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363a74a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
